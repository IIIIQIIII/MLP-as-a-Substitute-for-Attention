# MLP-as-a-Substitute-for-Attention
This repository presents a series of experiments demonstrating that MLP (Multi-Layer Perceptron), when sufficiently trained, can effectively substitute the attention mechanism in GPT models.
